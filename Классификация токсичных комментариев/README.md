# Классификация токсичных комментариев

## Инструменты и библиотеки:
![Python](https://img.shields.io/badge/-Python-white?style=flat&logo=python)
![Pandas](https://img.shields.io/badge/-Pandas-white?style=flat&logo=pandas&logoColor=130754)
![Nltk](https://img.shields.io/badge/-Nltk-white?style=flat&logo=Nltk)
![Spacy](https://img.shields.io/badge/-Spacy-white?style=flat&logo=Spacy)
![tf-idf](https://img.shields.io/badge/-tf_idf-white?style=flat&logo=tf-idf)
![CatBoost](https://img.shields.io/badge/-CatBoost-white?style=flat&logo=CatBoost)

## Описание проекта:
Интернет-магазин запускает новый сервис. 

**Задача исследования** — построить модель машинного обучения позволяющую классифицировать комментарии на позитивные и негативные.

В новом сервисе пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Требуется инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.

## Выводы:
В результате исследования были проанализированы данные о токсичности комментариев. Оказалось, что токсичных комментариев, около 10%. Комментарии были токенизированы и лемминизированы. Далее был создан список стоп слов, а данные векторизированны. В ходе обучения моделей и последующей проверки их на метрике f1 обнаружена оптимальная модель с гиперпараметрами: CatBoostClassifier(iterations=600, learning_rate=0.5, verbose=100, random_state=12345). Найденная модеь сможет эффективно находить токсичные комментарии
